## Response to Stevens Reading
#### In the readings for Tuesday and today (Stevens et al.) the authors use a technique to produce a high resolution description of the distribution of human populations across the globe. What is the name of the technique and describe in general and basic terms how it works?


The article, Disaggregating Census Data for Population Mapping Using Random Forests with Remotely-Sensed and Anciallary Data (Stevens et al.), they use a technique to produce the high resolution description of the distribution of human populations that the question refers to. The name of this technique is called "Random Forest Technique". First, the Random Forest models are an "ensemble, non parametric modeling approach", in which the data produces a "forest" of their own classifications, also known as regression trees, and this improves the issue of bagging by using the best of a random selecion. This technique also has an advantage which is having fewer "tuning parameters". So, how does this technique work? It first fits a series of models using the "tuneRF" function with every available covariate and log population density of each census unit as the response variable. Second, this step is a selection process for the resulting Random Forest, but it is also "very conservative" and uses the covariates. Then we use the resulting forest of trees and extract the "covariate importance scores". The next resulting RAndom Forest is used to predict on a scale of countries, with a very detailed map of log population densities. Lastly, the Random Forest model trees are distributed accordingly to one or more parallel processing environments and predictions are calculated for each pixel within the country. 

#### The random forest method used by the authors is a machine learning algorithm (ensemble method). In general terms, what is a machine learning algorithm? Within the context of this study what distinguishes a data science, machine learning method (such as random forest) from previous classical statistical approaches to describing and analyzing phenomenon and events?

A machine learning algorithm builds a mathematical model based on some sample data, so that it can make predictions without being programmed to perform that specific task. With classical statistical techniques, they are static in nature. They use one formula to send the sample data through, and they use the end result to predict factors. This approach, however, is not accurate for every data point in the sample and other factors. But with machine learning algorithms, it is possible to just let the computer use a wide variety of applications to accurately predict the results for each data point. When describing events and phenomenons, not every event is going to be the same as previous events, or even similar. With classical statistics, we can only pull from the most similar event that previously happened, and use that to describe the event. But this approach is not the most accurate and might even be not accurate at all. 

#### In the reading, the authors use a number of geospatial covariates as predictors in their machine learning method. What were these geospatial covariates and approximately how big of a data set did they represent (in general terms)? What is the significance of big data in the estimation of machine learning methods for inferring the correlates and drivers of human population distributions?

#### The authorsâ€™ results present a remarkable improvement over previous geospatial descriptions at very high resolution, of the distribution of the human population. Within the context of human development in LMICs, what is the significance of having a highly accurate description of where each person is located across planet earth?

#### Within the context of human development in LMICs, what is the relevance to your area of investigation in having a highly accurate description of where each household and person is located across planet earth?
