##### The End of Theory: The Data Deluge Makes the Scientific Method Obsolete (Anderson)

Models used to be able to predict consistently, or explain the world around us, until now. Companies like Google don’t settle for models. The petabyte Age. At the petabyte scale, information is not a simple three and four-dimensional taxonomy but of dimensionally agnostic statistics. It forces us to look at data as mathematics first then establish a context later. Better data, with better analytical tools wins. Peter Norvig, Google's research director: "all models are wrong, and increasingly you can succeed without them." massive amounts of data and applied math replace every other tool. With such massive data, the hypothesis method is becoming obsolete. Physics and biology have flawed models, just a simplification of Newton's laws. Petabytes says: correlation is enough.Stop looking for models. Throw the biggest numbers in a computer and let statistical algorithms find patterns that science cannot. Correlation supersedes causation, and science can advance even without coherent models, unified theories, or really any mechanistic explanation at all.

##### Big Data, new epistemologies and paradigm shifts (Kitchin)

Big Data and new data analytics challenges established epistemologies across the sciences, social sciences and humanities. They also assess how much they engender paradigm shifts. It explores new forms of empiricism that declare 'the end of theory', the creation of data-driven rather than knowledge-driven science. Big Data and new data analytics are disruptive innovations which reconfigure in many instances how research is conducted. There is an urgent need for critical reflection within the academy on epistemological implication of the data revolution. It is contended that a fruitful approach would be the development of a situated, reflexive, and contextually nuanced epistemology. 


##### Anderson describes what he calls the “Death of Theory”. What did he mean by this statement?In response to this, Kitchin offers a rebuttal to Anderson. What do you think? Has Big Data killed theory?
	
As our technology becomes more and more advanced, our data sets become larger in size as well. Sixty years ago, when computers first surfaced, we needed a way to store the memory, so we created floppy disks. These disks, however, could only store kilobytes of information. Compared to current times, that would be a word document or a list of numbers in Excel. Now, we have measurements such as petabytes, which are 10^12 times bigger than a kilobyte. So, as data becomes more robust in size, our traditional scientific method isn’t really viable to use when there are millions of data points in one experiment. It would take much more time, money, and effort for people to conduct it rather than a computer. Anderson argues that we shouldn’t be making models anymore because if it worked for a large company like Google, it should work for the rest of us too. He also posits that correlation supersedes causation. 
	
Whereas the census seeks to be exhaustive, enumerating all people living in a country, most surveys and other forms of data generation are samples, seeking to be representative of a population. In contrast, Big Data is characterized by being generated continuously, seeking to be exhaustive and fine-grained in scope, and flexible and scalable in its production. 
